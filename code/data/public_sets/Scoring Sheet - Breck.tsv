Characteristic	Subcharacteristics	Definition	External Practice Source	External Practice Name	External Practice Group	Score Annotator A	Score Annotator B
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	3	3
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	1	1
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	0	1
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	1
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	3	3
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	0	2
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	3	3
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	4	3
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	2	1
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	2	2
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	2	3
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	2
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	3	3
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	3	3
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	2	2
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	1
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	1	0
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	3	2
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	4	4
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Models are not too stale	MONITORING TESTS FOR ML	4	4
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Models are numerically stable	MONITORING TESTS FOR ML	2	3
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	1	2
Utility	Accuracy	The extent to which the ML system correctly predicts or models its target	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	4	4
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	2	0
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	1	0
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	1	0
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	1	0
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	1	0
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	3	3
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	1	0
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	3	4
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	3	2
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	4	3
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	2	2
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	2	2
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	2	2
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	1	1
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	2	2
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	2	2
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	3	3
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	3	3
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	2	2
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	2	2
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	2	1
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	1	3
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	3	3
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Models are not too stale	MONITORING TESTS FOR ML	4	2
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	2	0
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	2	0
Utility	Effectiveness	The ability of an ML system to produce a desired result on the business task it is being used for.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	3	2
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	3	2
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	3	3
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	1	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	1	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	3	2
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	3	2
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	2	2
Utility	Responsiveness	The ability of an ML system to complete the desired task in an acceptable time frame.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	2	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	2	2
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	2	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	3	3
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	2	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	2	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Models are not too stale	MONITORING TESTS FOR ML	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	1	0
Utility	Reusability	The ability to reuse the same ML system without any change, for another business case.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	1	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	3	3
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	4	4
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	1	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	3	3
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	2	1
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	1	1
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	2	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	3	2
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	3	2
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	3	3
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	2	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	1	1
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	2	1
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	1	1
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	1
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	1	1
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	1	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	1	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	3	3
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Models are not too stale	MONITORING TESTS FOR ML	3	3
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	2	2
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	3	3
Economy	Cost-effectiveness	The extent to which an ML System achieves the desired relationship between costs and overall impact.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	3	3
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	1	0
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	3	3
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	4	4
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	1	0
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	3	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	1	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	1	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	2	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	1	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	3	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	2	3
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	1	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	1	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	2	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	1	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	2	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	1	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	2	2
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	1	1
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	2	0
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Models are not too stale	MONITORING TESTS FOR ML	2	0
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	1	0
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	3	3
Economy	Efficiency	The ability to avoid wasting resources (computational, human, financial, etc.) in order to perform the desired task effectively.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	2	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	2	1
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	2	2
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	2	2
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	2	2
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	3	3
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	1	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	1	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	1	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	2	2
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	4	2
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	3	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	2	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	1	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Models are not too stale	MONITORING TESTS FOR ML	1	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	2	0
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	3	2
Robustness	Availability	The probability that the system will be up and running and able to deliver useful services to users.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	2	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	2	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	3	3
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	3	3
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	3	2
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	2	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	4	3
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	3	3
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	1	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	2	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	2	2
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	3	3
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	2	3
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	1	2
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	2
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	2	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	3	3
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	3	3
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	1	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Models are not too stale	MONITORING TESTS FOR ML	1	0
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	4	3
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	3	3
Robustness	Resilience	The extent to which an ML system can provide and maintain an acceptable level of service in the face of technical challenges to normal operation.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	1	2
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	1	2
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	1	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	2	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	2	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	1	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	2	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	1	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	3	3
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	2	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	1	2
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	0	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	0	2
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	1	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	1	1
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	1
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Models are not too stale	MONITORING TESTS FOR ML	4	4
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	2	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	1	0
Robustness	Adaptability	The extent to which an ML system can adapt to changes in the production environment, always providing the same functioning level.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	1	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	2	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	2	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	1	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	1	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	2	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	1	2
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	1	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	2	2
Robustness	Scalability	The extent of an ML system to handle a growing amount of work by adding resources to the system.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	1	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	2
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	1	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	4	4
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	3	3
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	3	3
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	1
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	2
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	2
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	3	3
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	3	3
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	3	3
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	3	2
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	2
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	1
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	1
Modifiability	Extensibility	The ease with which a system can be modified, in order to be used for another purpose.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	1
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	4	3
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	3	3
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	2	2
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	3	2
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	2	2
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	3	3
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	3	4
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	3	3
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	2	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	1	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	3	4
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	3	4
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	4	4
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	4	3
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	3	3
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	3	3
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	1	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	1	0
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	1	1
Modifiability	Maintainability	The ease with which activities aiming at keeping an ML system functional in the desired regime, can be performed.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	1	1
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	1	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	2	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	1	2
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	1	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	3	2
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	3	2
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	3	2
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	0
Modifiability	Modularity	The extent to which an ML system consists of components of limited functionality that interrelate with each other.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	4	3
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	2	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	1	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	2	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	1	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	4	4
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	3	2
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	3	1
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	2	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	3	4
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	3	4
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	2	1
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	4	3
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	2	2
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	2	2
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	0
Modifiability	Testability	The extent with which an ML system can be tested against expected behaviours.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	1	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	2	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	1	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	1	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	2	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	1	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	2	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	2	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	3	3
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	1	2
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	1	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	0
Productionizability	Deployability	The extent to which an ML system can be deployed in production when needed.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	1	2
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	2	2
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	2	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	1	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	2	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	1	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	4	4
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	3	3
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	3	4
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	2	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	1	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	1	2
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	1
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	1
Productionizability	Repeatability	The ease with which the ML lifecycle can be repeated.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	1
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	0	1
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	1	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	1	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	2	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	2	3
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	1	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	0
Productionizability	Operability	The extent to which an ML system can be controlled (disable, revert, upload new version, etc.).	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	1	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	2	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	1	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	1	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	2	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	4	3
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	4	3
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	4	3
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Models are not too stale	MONITORING TESTS FOR ML	4	3
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	4	3
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	4	3
Productionizability	Monitoring	The extent to which relevant indicators of an ML system are effectively observed/monitored and integrated in the operation of the system.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	4	3
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	3	2
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	0
Comprehensibility	Discoverability	The extent to which an ML system can be found (by means of performing a search in a database or any other information retrieval system).	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	2
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	2
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	3	2
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	2	2
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	1
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	3	2
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	1	1
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	0
Comprehensibility	Readability	The ease with which the code of an ML system can be understood.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	1	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	2	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	3	2
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	2	1
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	3	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	2	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	2	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	0
Comprehensibility	Traceability	The ability to relate artifacts created during the development of an ML system to how they were generated.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	3	3
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	1	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	1	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	3	3
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	3	3
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	2	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	1	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	1	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	1	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	3	2
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	2	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	1	0
Comprehensibility	Understandability	The ease with which the implementation and design choices of an ML system can be understood.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	2	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	3	3
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	0	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	1	1
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	0	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Models are not too stale	MONITORING TESTS FOR ML	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	1	0
Comprehensibility	Usability	The extent to which an ML system can be effectively used by users.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	1	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	2	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	2	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	2	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	2	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	1	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	1	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	4	4
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	4	4
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	3	2
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	2	2
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	1	1
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	1	1
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	2	2
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	4	4
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	4	4
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	2	2
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	4	4
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	1	0
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	3	2
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	2	2
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	1	2
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Models are not too stale	MONITORING TESTS FOR ML	2	2
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	2	2
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	2	2
Comprehensibility	Debuggability	The extent to which the inner workings of the ML system can be analyzed in order to understand why it behaves the way it does.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	2	2
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	2	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	2	1
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	2	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	1	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	2	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	1
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	1	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	1	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	0
Responsibility	Explainability	The ability to explain the output of an ML system.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	1	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	1	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	2	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	1	2
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	2	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	2	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	2	1
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	4	4
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	4	4
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	2	1
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	1	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	1	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Models are not too stale	MONITORING TESTS FOR ML	1	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	0
Responsibility	Fairness	The extent to which an ML system prevents unjust predictions towards protected attributes (race, gender, income, etc).	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	1	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	1	2
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Models are not too stale	MONITORING TESTS FOR ML	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	0	0
Responsibility	Ownership	The extent to which there exists people appointed to maintaining the ML System and supporting all the relevant stakeholders.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	2	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	2	3
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	4	4
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	1	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	3	3
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	2	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	3	3
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	1	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	1	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	1	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	1	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	0	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Models are not too stale	MONITORING TESTS FOR ML	1	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	1	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	1	0
Responsibility	Standards Compliance	The extent to which applicable standards are followed in the ML system.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	1	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Feature expectations are captured in a schema	TESTS FOR FEATURES AND DATA	2	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	All features are beneficial.	TESTS FOR FEATURES AND DATA	2	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	No feature’s cost is too much.	TESTS FOR FEATURES AND DATA	1	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Features adhere to meta-level requirements.	TESTS FOR FEATURES AND DATA	3	3
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	The data pipeline has appropriate privacy controls.	TESTS FOR FEATURES AND DATA	2	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	New features can be added quickly.	TESTS FOR FEATURES AND DATA	0	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	All input feature code is tested.	TESTS FOR FEATURES AND DATA	3	3
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Model specs are reviewed and submitted.	TESTS FOR MODEL DEVELOPMENT	2	1
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Offline and online metrics correlate.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	All hyperparameters have been tuned.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	The impact of model staleness is known.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	A simpler model is not better.	TESTS FOR MODEL DEVELOPMENT	1	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Model quality is sufficient on important data slices.	TESTS FOR MODEL DEVELOPMENT	1	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	The model is tested for considerations of inclusion.	TESTS FOR MODEL DEVELOPMENT	0	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Training is reproducible.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Model specs are unit tested.	TESTS FOR ML INFRASTRUCTURE	2	2
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	The ML pipeline is Integration tested	TESTS FOR ML INFRASTRUCTURE	2	2
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Model quality is validated before serving	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	The model is debuggable	TESTS FOR ML INFRASTRUCTURE	2	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Models are canaried before serving	TESTS FOR ML INFRASTRUCTURE	2	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Serving models can be rolled back.	TESTS FOR ML INFRASTRUCTURE	0	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Dependency changes result in notification	MONITORING TESTS FOR ML	1	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Data invariants hold for inputs	MONITORING TESTS FOR ML	1	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Training and serving are not skewed	MONITORING TESTS FOR ML	1	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Models are not too stale	MONITORING TESTS FOR ML	2	1
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Models are numerically stable	MONITORING TESTS FOR ML	2	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Computing performance has not regressed	MONITORING TESTS FOR ML	1	0
Responsibility	Vulnerability	The ease with which the system can be (ab)used to achieve malicious purposes.	Breck	Prediction quality has not regressed.	MONITORING TESTS FOR ML	2	0